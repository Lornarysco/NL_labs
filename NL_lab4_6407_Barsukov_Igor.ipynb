{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G2RQRgcfmdes"
   },
   "source": [
    "# Лабораторная работа 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6RXrbBbNmdev"
   },
   "source": [
    "Tensorflow 2.x\n",
    "\n",
    "1) Подготовка данных\n",
    "\n",
    "2) Использование Keras Model API\n",
    "\n",
    "3) Использование Keras Sequential + Functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.16.1-cp39-cp39-win_amd64.whl (2.1 kB)\n",
      "Collecting tensorflow-intel==2.16.1\n",
      "  Using cached tensorflow_intel-2.16.1-cp39-cp39-win_amd64.whl (376.9 MB)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\ibarsukov\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.4.0)\n",
      "Collecting tensorboard<2.17,>=2.16\n",
      "  Using cached tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ibarsukov\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (63.4.1)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Using cached grpcio-1.63.0-cp39-cp39-win_amd64.whl (3.9 MB)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\ibarsukov\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.16.0)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting h5py>=3.10.0\n",
      "  Using cached h5py-3.11.0-cp39-cp39-win_amd64.whl (3.0 MB)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1\n",
      "  Using cached gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\ibarsukov\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\ibarsukov\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.3.0)\n",
      "Collecting keras>=3.0.0\n",
      "  Using cached keras-3.3.3-py3-none-any.whl (1.1 MB)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\ibarsukov\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\ibarsukov\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\ibarsukov\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.31.0)\n",
      "Collecting ml-dtypes~=0.3.1\n",
      "  Using cached ml_dtypes-0.3.2-cp39-cp39-win_amd64.whl (127 kB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\ibarsukov\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\ibarsukov\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.25.3)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\ibarsukov\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\ibarsukov\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (21.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\ibarsukov\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.1->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: namex in c:\\users\\ibarsukov\\anaconda3\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\ibarsukov\\anaconda3\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.11.0)\n",
      "Collecting rich\n",
      "  Using cached rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\ibarsukov\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ibarsukov\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ibarsukov\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ibarsukov\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (3.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\ibarsukov\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\ibarsukov\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.0.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\ibarsukov\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\ibarsukov\\anaconda3\\lib\\site-packages (from packaging->tensorflow-intel==2.16.1->tensorflow) (3.0.9)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\ibarsukov\\anaconda3\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (2.17.2)\n",
      "Collecting markdown-it-py>=2.2.0\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Collecting mdurl~=0.1\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: opt-einsum, ml-dtypes, mdurl, h5py, grpcio, google-pasta, gast, astunparse, absl-py, tensorboard, markdown-it-py, rich, keras, tensorflow-intel, tensorflow\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 3.7.0\n",
      "    Uninstalling h5py-3.7.0:\n",
      "      Successfully uninstalled h5py-3.7.0\n",
      "Successfully installed absl-py-2.1.0 astunparse-1.6.3 gast-0.5.4 google-pasta-0.2.0 grpcio-1.63.0 h5py-3.11.0 keras-3.3.3 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.3.2 opt-einsum-3.3.0 rich-13.7.1 tensorboard-2.16.2 tensorflow-2.16.1 tensorflow-intel-2.16.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gUPFB3ZKmdew"
   },
   "source": [
    "https://www.tensorflow.org/tutorials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v-KLJDFXmdew"
   },
   "source": [
    "Для выполнения лабораторной работы необходимо установить tensorflow версии 2.0 или выше .\n",
    "\n",
    "Рекомендуется использовать возможности Colab'а по обучению моделей на GPU.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-21T10:32:01.394648Z",
     "start_time": "2024-03-21T10:32:01.378584Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1TSC3kHzmdex",
    "outputId": "578f857b-cd13-40b4-fd67-6becc266d599"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ibarsukov\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "USE_GPU = True\n",
    "\n",
    "if USE_GPU:\n",
    "    device = '/device:GPU:0'\n",
    "else:\n",
    "    device = '/cpu:0'\n",
    "\n",
    "# Constant to control how often we print when training models.\n",
    "print_every = 100\n",
    "print('Using device: ', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QW9-fCBumdey"
   },
   "source": [
    "# 1. Подготовка данных\n",
    "Загрузите набор данных из предыдущей лабораторной работы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-21T10:32:03.359832Z",
     "start_time": "2024-03-21T10:32:01.709774Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mLLN4mzDmdez",
    "outputId": "03032cc7-cfb9-49a1-858e-483c07991c7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n",
      "Train data shape:  (49000, 28, 28, 1)\n",
      "Train labels shape:  (49000,) int32\n",
      "Validation data shape:  (1000, 28, 28, 1)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (10000, 28, 28, 1)\n",
      "Test labels shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "def load_mnist(num_training=49000, num_validation=1000, num_test=10000):\n",
    "    \"\"\"\n",
    "    Fetch the CIFAR-10 dataset from the web and perform preprocessing to prepare\n",
    "    it for the two-layer neural net classifier. These are the same steps as\n",
    "    we used for the SVM, but condensed to a single function.\n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 dataset and use appropriate data types and shapes\n",
    "    mnist = tf.keras.datasets.mnist.load_data()\n",
    "    (X_train, y_train), (X_test, y_test) = mnist\n",
    "    X_train = np.asarray(X_train, dtype=np.float32)\n",
    "    y_train = np.asarray(y_train, dtype=np.int32).flatten()\n",
    "    X_test = np.asarray(X_test, dtype=np.float32)\n",
    "    y_test = np.asarray(y_test, dtype=np.int32).flatten()\n",
    "\n",
    "    # Subsample the data\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "\n",
    "    # Normalize the data: subtract the mean pixel and divide by std\n",
    "    mean_pixel = X_train.mean(axis=(0, 1, 2), keepdims=True)\n",
    "    std_pixel = X_train.std(axis=(0, 1, 2), keepdims=True)\n",
    "    X_train = (X_train - mean_pixel) / std_pixel\n",
    "    X_val = (X_val - mean_pixel) / std_pixel\n",
    "    X_test = (X_test - mean_pixel) / std_pixel\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "# If there are errors with SSL downloading involving self-signed certificates,\n",
    "# it may be that your Python version was recently installed on the current machine.\n",
    "# See: https://github.com/tensorflow/tensorflow/issues/10779\n",
    "# To fix, run the command: /Applications/Python\\ 3.7/Install\\ Certificates.command\n",
    "#   ...replacing paths as necessary.\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "NHW = (0, 1, 2)\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_mnist()\n",
    "X_train = X_train.reshape(-1, 28, 28, 1)\n",
    "X_val = X_val.reshape(-1, 28, 28, 1)\n",
    "X_test = X_test.reshape(-1, 28, 28, 1)\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape, y_train.dtype)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-21T10:32:03.390892Z",
     "start_time": "2024-03-21T10:32:03.361829Z"
    },
    "id": "zFdFh2Tsmdez"
   },
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    def __init__(self, X, y, batch_size, shuffle=False):\n",
    "        \"\"\"\n",
    "        Construct a Dataset object to iterate over data X and labels y\n",
    "\n",
    "        Inputs:\n",
    "        - X: Numpy array of data, of any shape\n",
    "        - y: Numpy array of labels, of any shape but with y.shape[0] == X.shape[0]\n",
    "        - batch_size: Integer giving number of elements per minibatch\n",
    "        - shuffle: (optional) Boolean, whether to shuffle the data on each epoch\n",
    "        \"\"\"\n",
    "        assert X.shape[0] == y.shape[0], 'Got different numbers of data and labels'\n",
    "        self.X, self.y = X, y\n",
    "        self.batch_size, self.shuffle = batch_size, shuffle\n",
    "\n",
    "    def __iter__(self):\n",
    "        N, B = self.X.shape[0], self.batch_size\n",
    "        idxs = np.arange(N)\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(idxs)\n",
    "        return iter((self.X[i:i+B], self.y[i:i+B]) for i in range(0, N, B))\n",
    "\n",
    "\n",
    "train_dset = Dataset(X_train, y_train, batch_size=64, shuffle=True)\n",
    "val_dset = Dataset(X_val, y_val, batch_size=64, shuffle=False)\n",
    "test_dset = Dataset(X_test, y_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-21T10:32:03.438893Z",
     "start_time": "2024-03-21T10:32:03.392899Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WxhBcG5Jmde0",
    "outputId": "ada0f915-0842-48ae-ebc0-ed17a52b26d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (64, 28, 28, 1) (64,)\n",
      "1 (64, 28, 28, 1) (64,)\n",
      "2 (64, 28, 28, 1) (64,)\n",
      "3 (64, 28, 28, 1) (64,)\n",
      "4 (64, 28, 28, 1) (64,)\n",
      "5 (64, 28, 28, 1) (64,)\n",
      "6 (64, 28, 28, 1) (64,)\n"
     ]
    }
   ],
   "source": [
    "# We can iterate through a dataset like this:\n",
    "for t, (x, y) in enumerate(train_dset):\n",
    "    print(t, x.shape, y.shape)\n",
    "    if t > 5: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7bBfBfYpmde1"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y3Pofr7mmde1"
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1UqJbbQsmde1"
   },
   "source": [
    "#  2. Keras Model Subclassing API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zMW3obRHmde1"
   },
   "source": [
    "\n",
    "Для реализации собственной модели с помощью Keras Model Subclassing API необходимо выполнить следующие шаги:\n",
    "\n",
    "1) Определить новый класс, который является наследником tf.keras.Model.\n",
    "\n",
    "2) В методе __init__() определить все необходимые слои из модуля tf.keras.layer\n",
    "\n",
    "3) Реализовать прямой проход в методе call() на основе слоев, объявленных в __init__()\n",
    "\n",
    "Ниже приведен пример использования keras API для определения двухслойной полносвязной сети.\n",
    "\n",
    "https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-21T10:32:03.469890Z",
     "start_time": "2024-03-21T10:32:03.440894Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-HXiyu_mmde1",
    "outputId": "f5d467d7-6c38-4a41-c8c3-23588653cc5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 10)\n"
     ]
    }
   ],
   "source": [
    "class TwoLayerFC(tf.keras.Model):\n",
    "    def __init__(self, hidden_size, num_classes):\n",
    "        super(TwoLayerFC, self).__init__()\n",
    "        initializer = tf.initializers.VarianceScaling(scale=2.0)\n",
    "        self.fc1 = tf.keras.layers.Dense(hidden_size, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc2 = tf.keras.layers.Dense(num_classes, activation='softmax',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def test_TwoLayerFC():\n",
    "    \"\"\" A small unit test to exercise the TwoLayerFC model above. \"\"\"\n",
    "    input_size, hidden_size, num_classes = 50, 42, 10\n",
    "    x = tf.zeros((64, input_size))\n",
    "    model = TwoLayerFC(hidden_size, num_classes)\n",
    "    # with tf.device(device):\n",
    "    scores = model(x)\n",
    "    print(scores.shape)\n",
    "\n",
    "test_TwoLayerFC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v76VwT_cmde2"
   },
   "source": [
    "Реализуйте трехслойную CNN для вашей задачи классификации.\n",
    "\n",
    "Архитектура сети:\n",
    "    \n",
    "1. Сверточный слой (5 x 5 kernels, zero-padding = 'same')\n",
    "2. Функция активации ReLU\n",
    "3. Сверточный слой (3 x 3 kernels, zero-padding = 'same')\n",
    "4. Функция активации ReLU\n",
    "5. Полносвязный слой\n",
    "6. Функция активации Softmax\n",
    "\n",
    "https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/Conv2D\n",
    "\n",
    "https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-21T10:32:03.485889Z",
     "start_time": "2024-03-21T10:32:03.471890Z"
    },
    "id": "9V4lPunEmde2"
   },
   "outputs": [],
   "source": [
    "class ThreeLayerConvNet(tf.keras.Model):\n",
    "    def __init__(self, channel_1, channel_2, num_classes):\n",
    "        super(ThreeLayerConvNet, self).__init__()\n",
    "        ########################################################################\n",
    "        # Определение слоев для сверточной нейронной сети.\n",
    "        ########################################################################\n",
    "        self.conv1 = tf.keras.layers.Conv2D(channel_1, (5, 5), padding='same', activation='relu')\n",
    "        self.conv2 = tf.keras.layers.Conv2D(channel_2, (3, 3), padding='same', activation='relu')\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.fc = tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        ########################################################################\n",
    "        # Прямой проход для сверточной нейронной сети.\n",
    "        ########################################################################\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-21T10:32:03.533896Z",
     "start_time": "2024-03-21T10:32:03.511895Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-R34I4KXmde2",
    "outputId": "f5eaa650-713d-4b68-801b-7164814bfd19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 10)\n"
     ]
    }
   ],
   "source": [
    "def test_ThreeLayerConvNet():\n",
    "    channel_1, channel_2, num_classes = 12, 8, 10\n",
    "    model = ThreeLayerConvNet(channel_1, channel_2, num_classes)\n",
    "    with tf.device(device):\n",
    "        x = tf.zeros((64, 3, 32, 32))\n",
    "        scores = model(x)\n",
    "        print(scores.shape)\n",
    "\n",
    "test_ThreeLayerConvNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BBh2BMKzmde2"
   },
   "source": [
    "Пример реализации процесса обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-21T10:32:03.581893Z",
     "start_time": "2024-03-21T10:32:03.562890Z"
    },
    "id": "OxOMn46Bmde3"
   },
   "outputs": [],
   "source": [
    "def train_part34(model_init_fn, optimizer_init_fn, num_epochs=1, is_training=False):\n",
    "    \"\"\"\n",
    "    Simple training loop for use with models defined using tf.keras. It trains\n",
    "    a model for one epoch on the CIFAR-10 training set and periodically checks\n",
    "    accuracy on the CIFAR-10 validation set.\n",
    "\n",
    "    Inputs:\n",
    "    - model_init_fn: A function that takes no parameters; when called it\n",
    "      constructs the model we want to train: model = model_init_fn()\n",
    "    - optimizer_init_fn: A function which takes no parameters; when called it\n",
    "      constructs the Optimizer object we will use to optimize the model:\n",
    "      optimizer = optimizer_init_fn()\n",
    "    - num_epochs: The number of epochs to train for\n",
    "\n",
    "    Returns: Nothing, but prints progress during trainingn\n",
    "    \"\"\"\n",
    "    with tf.device(device):\n",
    "\n",
    "        # Compute the loss like we did in Part II\n",
    "        loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "        model = model_init_fn()\n",
    "        optimizer = optimizer_init_fn()\n",
    "\n",
    "        train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "        train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "        val_loss = tf.keras.metrics.Mean(name='val_loss')\n",
    "        val_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='val_accuracy')\n",
    "\n",
    "        t = 0\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            for x_np, y_np in train_dset:\n",
    "                with tf.GradientTape() as tape:\n",
    "\n",
    "                    # Use the model function to build the forward pass.\n",
    "                    scores = model(x_np, training=is_training)\n",
    "                    loss = loss_fn(y_np, scores)\n",
    "\n",
    "                    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "                    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "                    # Update the metrics\n",
    "                    train_loss.update_state(loss)\n",
    "                    train_accuracy.update_state(y_np, scores)\n",
    "\n",
    "                    if t % print_every == 0:\n",
    "                        for test_x, test_y in val_dset:\n",
    "                            # During validation at end of epoch, training set to False\n",
    "                            prediction = model(test_x, training=False)\n",
    "                            t_loss = loss_fn(test_y, prediction)\n",
    "\n",
    "                            val_loss.update_state(t_loss)\n",
    "                            val_accuracy.update_state(test_y, prediction)\n",
    "\n",
    "                        template = 'Iteration {}, Epoch {}, Loss: {}, Accuracy: {}, Val Loss: {}, Val Accuracy: {}'\n",
    "                        print (template.format(t, epoch+1,\n",
    "                                             train_loss.result(),\n",
    "                                             train_accuracy.result()*100,\n",
    "                                             val_loss.result(),\n",
    "                                             val_accuracy.result()*100))\n",
    "                    t += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-21T10:32:34.721805Z",
     "start_time": "2024-03-21T10:32:03.751891Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AKjdcmMhmde3",
    "outputId": "02d779ae-ad02-4d32-ff19-efd42d344c3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 2.9824066162109375, Accuracy: 4.6875, Val Loss: 2.6850626468658447, Val Accuracy: 28.400001525878906\n",
      "Iteration 100, Epoch 1, Loss: 0.6382007598876953, Accuracy: 80.49195098876953, Val Loss: 1.6128334999084473, Val Accuracy: 55.900001525878906\n",
      "Iteration 200, Epoch 1, Loss: 0.5087659955024719, Accuracy: 84.6237564086914, Val Loss: 1.2252578735351562, Val Accuracy: 66.19999694824219\n",
      "Iteration 300, Epoch 1, Loss: 0.45031145215034485, Accuracy: 86.5188980102539, Val Loss: 1.0207841396331787, Val Accuracy: 71.4749984741211\n",
      "Iteration 400, Epoch 1, Loss: 0.4061107039451599, Accuracy: 87.83899688720703, Val Loss: 0.88978111743927, Val Accuracy: 75.05999755859375\n",
      "Iteration 500, Epoch 1, Loss: 0.3826177716255188, Accuracy: 88.51671600341797, Val Loss: 0.8009709715843201, Val Accuracy: 77.43333435058594\n",
      "Iteration 600, Epoch 1, Loss: 0.3593593239784241, Accuracy: 89.23149108886719, Val Loss: 0.7360186576843262, Val Accuracy: 79.25714111328125\n",
      "Iteration 700, Epoch 1, Loss: 0.3427359461784363, Accuracy: 89.75125122070312, Val Loss: 0.6827762126922607, Val Accuracy: 80.5999984741211\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hidden_size, num_classes = 4000, 10\n",
    "learning_rate = 1e-2\n",
    "\n",
    "def model_init_fn():\n",
    "    return TwoLayerFC(hidden_size, num_classes)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    return tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jjGVqPb_mde3"
   },
   "source": [
    "Обучите трехслойную CNN. В tf.keras.optimizers.SGD укажите Nesterov momentum = 0.9 .\n",
    "\n",
    "https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/optimizers/SGD\n",
    "\n",
    "Значение accuracy на валидационной выборке после 1 эпохи обучения должно быть > 50% ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-21T10:33:26.734189Z",
     "start_time": "2024-03-21T10:32:34.723805Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q-64ZL0mmde3",
    "outputId": "e1e7b50f-9e9c-4340-91da-d0ade64b3b8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 2.3644020557403564, Accuracy: 7.8125, Val Loss: 2.314464807510376, Val Accuracy: 13.600000381469727\n",
      "Iteration 100, Epoch 1, Loss: 0.6915977597236633, Accuracy: 80.07425689697266, Val Loss: 1.396274209022522, Val Accuracy: 49.20000076293945\n",
      "Iteration 200, Epoch 1, Loss: 0.5153015851974487, Accuracy: 85.19900512695312, Val Loss: 1.0643095970153809, Val Accuracy: 62.13333511352539\n",
      "Iteration 300, Epoch 1, Loss: 0.4213019907474518, Accuracy: 87.80107879638672, Val Loss: 0.8568112850189209, Val Accuracy: 69.80000305175781\n",
      "Iteration 400, Epoch 1, Loss: 0.3549521267414093, Accuracy: 89.67035675048828, Val Loss: 0.7282220125198364, Val Accuracy: 74.54000091552734\n",
      "Iteration 500, Epoch 1, Loss: 0.3159732222557068, Accuracy: 90.82460021972656, Val Loss: 0.6378588080406189, Val Accuracy: 77.76667022705078\n",
      "Iteration 600, Epoch 1, Loss: 0.28400829434394836, Accuracy: 91.74552917480469, Val Loss: 0.5744606852531433, Val Accuracy: 80.02857208251953\n",
      "Iteration 700, Epoch 1, Loss: 0.25942763686180115, Accuracy: 92.44159698486328, Val Loss: 0.5214881896972656, Val Accuracy: 81.875\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 3e-3\n",
    "channel_1, channel_2, num_classes = 32, 16, 10\n",
    "\n",
    "def model_init_fn():\n",
    "    model = None\n",
    "    ############################################################################\n",
    "    # TODO: Complete the implementation of model_fn.                           #\n",
    "    ############################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    model = ThreeLayerConvNet(channel_1, channel_2, num_classes)\n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    ############################################################################\n",
    "    #                           END OF YOUR CODE                               #\n",
    "    ############################################################################\n",
    "    return model\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    optimizer = None\n",
    "    ############################################################################\n",
    "    # TODO: Complete the implementation of model_fn.                           #\n",
    "    ############################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9, nesterov=True)\n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    ############################################################################\n",
    "    #                           END OF YOUR CODE                               #\n",
    "    ############################################################################\n",
    "    return optimizer\n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KPvuYgDWmde4"
   },
   "source": [
    "# 3. Использование Keras Sequential API для реализации последовательных моделей.\n",
    "\n",
    "Пример для полносвязной сети:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-21T10:33:55.035281Z",
     "start_time": "2024-03-21T10:33:26.735190Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qjmdhDmSmde4",
    "outputId": "3e787cd1-d8c9-43be-ff8f-bd4378ff85e2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ibarsukov\\anaconda3\\lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 2.7539634704589844, Accuracy: 12.5, Val Loss: 2.545178174972534, Val Accuracy: 19.69999885559082\n",
      "Iteration 100, Epoch 1, Loss: 0.6406766772270203, Accuracy: 80.43006896972656, Val Loss: 1.5581419467926025, Val Accuracy: 51.150001525878906\n",
      "Iteration 200, Epoch 1, Loss: 0.5121080875396729, Accuracy: 84.78700256347656, Val Loss: 1.1870709657669067, Val Accuracy: 62.83333206176758\n",
      "Iteration 300, Epoch 1, Loss: 0.45540326833724976, Accuracy: 86.5188980102539, Val Loss: 0.9948983192443848, Val Accuracy: 68.75\n",
      "Iteration 400, Epoch 1, Loss: 0.41165563464164734, Accuracy: 87.81562042236328, Val Loss: 0.8688325881958008, Val Accuracy: 72.68000030517578\n",
      "Iteration 500, Epoch 1, Loss: 0.38773313164711, Accuracy: 88.4886474609375, Val Loss: 0.7833664417266846, Val Accuracy: 75.53333282470703\n",
      "Iteration 600, Epoch 1, Loss: 0.36417245864868164, Accuracy: 89.19769287109375, Val Loss: 0.7193476557731628, Val Accuracy: 77.68571472167969\n",
      "Iteration 700, Epoch 1, Loss: 0.3464435040950775, Accuracy: 89.75347900390625, Val Loss: 0.6690555810928345, Val Accuracy: 79.3375015258789\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-2\n",
    "\n",
    "def model_init_fn():\n",
    "    input_shape = (28,28,1)\n",
    "    hidden_layer_size, num_classes = 4000, 10\n",
    "    initializer = tf.initializers.VarianceScaling(scale=2.0)\n",
    "    layers = [\n",
    "        tf.keras.layers.Flatten(input_shape=input_shape),\n",
    "        tf.keras.layers.Dense(hidden_layer_size, activation='relu',\n",
    "                              kernel_initializer=initializer),\n",
    "        tf.keras.layers.Dense(num_classes, activation='softmax',\n",
    "                              kernel_initializer=initializer),\n",
    "    ]\n",
    "    model = tf.keras.Sequential(layers)\n",
    "    return model\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    return tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nvijUA_9mde4"
   },
   "source": [
    "Альтернативный менее гибкий способ обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-21T10:33:59.392586Z",
     "start_time": "2024-03-21T10:33:55.037282Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JJzOLRM2mde4",
    "outputId": "4f02a80c-b579-48ef-ff49-820d35067526"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m766/766\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 21ms/step - loss: 0.5400 - sparse_categorical_accuracy: 0.8380 - val_loss: 0.3001 - val_sparse_categorical_accuracy: 0.9140\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.2484 - sparse_categorical_accuracy: 0.9315\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.21309392154216766, 0.9413999915122986]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model_init_fn()\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=learning_rate),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=[tf.keras.metrics.sparse_categorical_accuracy])\n",
    "model.fit(X_train, y_train, batch_size=64, epochs=1, validation_data=(X_val, y_val))\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UtXOP9immde4"
   },
   "source": [
    "Перепишите реализацию трехслойной CNN с помощью tf.keras.Sequential API . Обучите модель двумя способами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-21T10:34:49.743065Z",
     "start_time": "2024-03-21T10:33:59.393586Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xv4nD7N3mde5",
    "outputId": "4e0ddd89-08b3-4abb-f062-d1a26ed46df1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 2.300825595855713, Accuracy: 14.0625, Val Loss: 2.3106460571289062, Val Accuracy: 13.199999809265137\n",
      "Iteration 100, Epoch 1, Loss: 2.2837440967559814, Accuracy: 18.193069458007812, Val Loss: 2.289423942565918, Val Accuracy: 16.799999237060547\n",
      "Iteration 200, Epoch 1, Loss: 2.2611896991729736, Accuracy: 24.269277572631836, Val Loss: 2.2663626670837402, Val Accuracy: 22.30000114440918\n",
      "Iteration 300, Epoch 1, Loss: 2.2347161769866943, Accuracy: 30.922964096069336, Val Loss: 2.238532304763794, Val Accuracy: 28.650001525878906\n",
      "Iteration 400, Epoch 1, Loss: 2.195530652999878, Accuracy: 37.620792388916016, Val Loss: 2.2000677585601807, Val Accuracy: 34.400001525878906\n",
      "Iteration 500, Epoch 1, Loss: 2.1464176177978516, Accuracy: 43.04827880859375, Val Loss: 2.1462247371673584, Val Accuracy: 39.53333282470703\n",
      "Iteration 600, Epoch 1, Loss: 2.0739898681640625, Accuracy: 47.894134521484375, Val Loss: 2.070671319961548, Val Accuracy: 43.757144927978516\n",
      "Iteration 700, Epoch 1, Loss: 1.9786055088043213, Accuracy: 51.8455810546875, Val Loss: 1.9743657112121582, Val Accuracy: 47.525001525878906\n"
     ]
    }
   ],
   "source": [
    "def model_init_fn():\n",
    "    model = None\n",
    "    ############################################################################\n",
    "    # TODO: Construct a three-layer ConvNet using tf.keras.Sequential.         #\n",
    "    ############################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    model = ThreeLayerConvNet(channel_1, channel_2, num_classes)\n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    ############################################################################\n",
    "    #                            END OF YOUR CODE                              #\n",
    "    ############################################################################\n",
    "    return model\n",
    "\n",
    "learning_rate = 5e-4\n",
    "def optimizer_init_fn():\n",
    "    optimizer = None\n",
    "    ############################################################################\n",
    "    # TODO: Complete the implementation of model_fn.                           #\n",
    "    ############################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate)\n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    ############################################################################\n",
    "    #                           END OF YOUR CODE                               #\n",
    "    ############################################################################\n",
    "    return optimizer\n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-21T10:34:49.744065Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FIRJ96Exmde5",
    "is_executing": true,
    "outputId": "91652cea-40aa-4412-fd86-abdb37c41952"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m766/766\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 26ms/step - loss: 0.6588 - sparse_categorical_accuracy: 0.8012 - val_loss: 0.2645 - val_sparse_categorical_accuracy: 0.9270\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.2016 - sparse_categorical_accuracy: 0.9420\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.17547741532325745, 0.9501000046730042]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model_init_fn()\n",
    "model.compile(optimizer='sgd',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=[tf.keras.metrics.sparse_categorical_accuracy])\n",
    "model.fit(X_train, y_train, batch_size=64, epochs=1, validation_data=(X_val, y_val))\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rMHQrrbzmde5"
   },
   "source": [
    "# 4. Использование Keras Functional API\n",
    "\n",
    "Для реализации более сложных архитектур сети с несколькими входами/выходами, повторным использованием слоев, \"остаточными\" связями (residual connections) необходимо явно указать входные и выходные тензоры.\n",
    "\n",
    "Ниже представлен пример для полносвязной сети."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CdKAP4Gjmde5",
    "outputId": "f6ec5eb0-218e-42ae-e523-4b55dd22b0f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 10)\n"
     ]
    }
   ],
   "source": [
    "def two_layer_fc_functional(input_shape, hidden_size, num_classes):\n",
    "    initializer = tf.initializers.VarianceScaling(scale=2.0)\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    flattened_inputs = tf.keras.layers.Flatten()(inputs)\n",
    "    fc1_output = tf.keras.layers.Dense(hidden_size, activation='relu',\n",
    "                                 kernel_initializer=initializer)(flattened_inputs)\n",
    "    scores = tf.keras.layers.Dense(num_classes, activation='softmax',\n",
    "                             kernel_initializer=initializer)(fc1_output)\n",
    "\n",
    "    # Instantiate the model given inputs and outputs.\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=scores)\n",
    "    return model\n",
    "\n",
    "def test_two_layer_fc_functional():\n",
    "    \"\"\" A small unit test to exercise the TwoLayerFC model above. \"\"\"\n",
    "    input_size, hidden_size, num_classes = 50, 42, 10\n",
    "    input_shape = (50,)\n",
    "\n",
    "    x = tf.zeros((64, input_size))\n",
    "    model = two_layer_fc_functional(input_shape, hidden_size, num_classes)\n",
    "\n",
    "    with tf.device(device):\n",
    "        scores = model(x)\n",
    "        print(scores.shape)\n",
    "\n",
    "test_two_layer_fc_functional()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wCnxBWsxmde5",
    "outputId": "36a11af2-e38d-4ca3-9e32-0e837a891c07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 3.3666727542877197, Accuracy: 10.9375, Val Loss: 2.891230583190918, Val Accuracy: 15.09999942779541\n",
      "Iteration 100, Epoch 1, Loss: 0.6526246070861816, Accuracy: 80.21348571777344, Val Loss: 1.7184056043624878, Val Accuracy: 48.5\n",
      "Iteration 200, Epoch 1, Loss: 0.5176721811294556, Accuracy: 84.56934356689453, Val Loss: 1.296210765838623, Val Accuracy: 60.9666633605957\n",
      "Iteration 300, Epoch 1, Loss: 0.4561094343662262, Accuracy: 86.53446960449219, Val Loss: 1.0756281614303589, Val Accuracy: 67.44999694824219\n",
      "Iteration 400, Epoch 1, Loss: 0.4094807803630829, Accuracy: 87.98316955566406, Val Loss: 0.9330778121948242, Val Accuracy: 71.73999786376953\n",
      "Iteration 500, Epoch 1, Loss: 0.38618385791778564, Accuracy: 88.63211059570312, Val Loss: 0.8358957767486572, Val Accuracy: 74.63333892822266\n",
      "Iteration 600, Epoch 1, Loss: 0.36247825622558594, Accuracy: 89.32768249511719, Val Loss: 0.7654280066490173, Val Accuracy: 76.81428527832031\n",
      "Iteration 700, Epoch 1, Loss: 0.3452050983905792, Accuracy: 89.869384765625, Val Loss: 0.7082769274711609, Val Accuracy: 78.5625\n"
     ]
    }
   ],
   "source": [
    "input_shape = (28, 28, 1)\n",
    "hidden_size, num_classes = 4000, 10\n",
    "learning_rate = 1e-2\n",
    "\n",
    "def model_init_fn():\n",
    "    return two_layer_fc_functional(input_shape, hidden_size, num_classes)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    return tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ejJunY24mde6"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F9Gcduahmde6"
   },
   "source": [
    "Поэкспериментируйте с архитектурой сверточной сети. Для вашего набора данных вам необходимо получить как минимум 70% accuracy на валидационной выборке за 10 эпох обучения. Опишите все эксперименты и сделайте выводы (без выполнения данного пункта работы приниматься не будут).\n",
    "\n",
    "Эспериментируйте с архитектурой, гиперпараметрами, функцией потерь, регуляризацией, методом оптимизации.  \n",
    "\n",
    "https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/BatchNormalization#methods https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/Dropout#methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "dyKT825Imde6"
   },
   "outputs": [],
   "source": [
    "class _IdentityBlock(tf.keras.Model):\n",
    "    \"\"\"Identity block utilizing skip connections.\"\"\"\n",
    "\n",
    "    def __init__(self, out_channels):\n",
    "        super().__init__()\n",
    "        \"\"\"Initializes the identiy block.\n",
    "\n",
    "        Here we simply initialize 2 layers which process the input and after the output\n",
    "        is produces it is added together with the input whcih is the final output.\n",
    "\n",
    "        Args:\n",
    "            out_channels (int): The number of activation maps this block should produce\n",
    "        \"\"\"\n",
    "        # Acts as Kaiming weight initalization\n",
    "        initializer = tf.initializers.VarianceScaling(scale=2.0)\n",
    "\n",
    "        # Part 1 of the convolution, normalization and non-linearity\n",
    "        self.conv1 = tf.keras.layers.Conv2D(out_channels, 3, padding='same', use_bias=False, kernel_initializer=initializer)\n",
    "        self.norm1 = tf.keras.layers.BatchNormalization(axis=3)\n",
    "        self.relu1 = tf.keras.layers.Activation('relu')\n",
    "\n",
    "        # Part 2 of the convolution, normalization and non-linearity\n",
    "        self.conv2 = tf.keras.layers.Conv2D(out_channels, 3, padding='same', use_bias=False, kernel_initializer=initializer)\n",
    "        self.norm2 = tf.keras.layers.BatchNormalization(axis=3)\n",
    "        self.relu2 = tf.keras.layers.Activation('relu')\n",
    "\n",
    "        # Add layer will add together the input and the output\n",
    "        self.add = tf.keras.layers.Add()\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        \"\"\"Performs forward pass on the given input.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor):      The input of dimensions (N, H, W, C)\n",
    "            training (bool): Indicates whether the forward pass happens in the training mode\n",
    "\n",
    "        Returns:\n",
    "              out (Tensor): Output data of dim (N, H, W, C)\n",
    "        \"\"\"\n",
    "        x_skip = tf.identity(x)                   # prepare to add the input to the output\n",
    "        x = self.relu1(self.norm1(self.conv1(x))) # pass input through the first layer\n",
    "        x = self.norm2(self.conv2(x))             # pass input through the second layer (without ReLU)\n",
    "        out = self.relu2(self.add([x, x_skip]))   # perform ReLU on the processed input added with the raw input\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, in_channels=32, block_config=(2, 2, 2, 2), num_classes=10):\n",
    "        \"\"\"Initializes the residual network.\n",
    "\n",
    "        The first layer produces `in_channels` activation maps which are then fed to a\n",
    "        sequence of blocks containing a specified number of identity sub-blocks (first\n",
    "        block is always *_BottleneckBlock*). At the end the _global average pooling_\n",
    "        layer is used to flatten the activations for the linear softmax classifier.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int):    The number of channels to extract after the first convolution\n",
    "            block_config (tuple): The number of layers each bloack should have in sequence\n",
    "            num_classes (int):    The total number of classes\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Acts as Kaiming weight initalization\n",
    "        initializer = tf.initializers.VarianceScaling(scale=2.0)\n",
    "\n",
    "        # Prepare the input for the chains of identity blocks\n",
    "        self.features = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(in_channels, 5, padding='same', use_bias=False, kernel_initializer=initializer),\n",
    "            tf.keras.layers.BatchNormalization(axis=3),\n",
    "            tf.keras.layers.Activation('relu'),\n",
    "        ])\n",
    "\n",
    "        num_features = in_channels # num feaure maps to produce after each group of identity blocks\n",
    "\n",
    "        # Loop through every group of blocks\n",
    "        for i, num_layers in enumerate(block_config):\n",
    "            # Use bottleneck block as the first block in every group (except first)\n",
    "            if i != 0:\n",
    "                self.features.add(_BottleneckBlock(num_features))\n",
    "            else:\n",
    "                self.features.add(_IdentityBlock(num_features))\n",
    "\n",
    "            # Create the specified number of identity blocks for i'th group\n",
    "            for j in range(num_layers-1):\n",
    "                self.features.add(_IdentityBlock(num_features))\n",
    "\n",
    "            num_features *= 2 # increase the nuber of features to be produced\n",
    "\n",
    "        # Flatten the final activation maps using global average pooling\n",
    "        self.features.add(tf.keras.layers.GlobalAveragePooling2D())\n",
    "\n",
    "        # Softmax classifier is used as the final layer\n",
    "        self.classifier = tf.keras.layers.Dense(num_classes, activation='softmax', kernel_initializer=initializer)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        \"\"\"Performs forward pass on the given input.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor):      The input of dimensions (N, H, W, C)\n",
    "            training (bool): Indicates whether the forward pass happens in the training mode\n",
    "\n",
    "        Returns:\n",
    "            out (Tensor): Output data of dim (N, 10)\n",
    "        \"\"\"\n",
    "        out = self.features(x)     # Get the extracted features for the linear classfier\n",
    "        out = self.classifier(out) # Perform classification with softmax activation\n",
    "\n",
    "        return out\n",
    "class _BottleneckBlock(tf.keras.Model):\n",
    "    \"\"\"Same as identity block except it reduces the spacial area before processing the input.\"\"\"\n",
    "\n",
    "    def __init__(self, out_channels):\n",
    "        \"\"\"Initializes the bottleneck block.\n",
    "\n",
    "        Unlike *_IdentityBlock*, the first convolution here reduces the spacial size of the input\n",
    "        by a factor of `2`. Then, it performs the main convolution after which the output maps\n",
    "        are added together with the input maps to produce final activations.\n",
    "\n",
    "        Args:\n",
    "            out_channels (int): The number of activation maps this block should produce\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Acts as Kaiming weight initalization\n",
    "        initializer = tf.initializers.VarianceScaling(scale=2.0)\n",
    "\n",
    "        # Reduce the input size by 2 to match output size\n",
    "        self.skip1 = tf.keras.layers.Conv2D(out_channels, 1, strides=2, use_bias=False, kernel_initializer=initializer)\n",
    "\n",
    "        # Part 1 of the convolution which reduces the spacial area\n",
    "        self.conv1 = tf.keras.layers.Conv2D(out_channels, 3, strides=2, padding='same', use_bias=False, kernel_initializer=initializer)\n",
    "        self.norm1 = tf.keras.layers.BatchNormalization(axis=3)\n",
    "        self.relu1 = tf.keras.layers.Activation('relu')\n",
    "\n",
    "        # Part 2 of the convolution which extracts features from the reduced input\n",
    "        self.conv2 = tf.keras.layers.Conv2D(out_channels, 3, padding='same', use_bias=False, kernel_initializer=initializer)\n",
    "        self.norm2 = tf.keras.layers.BatchNormalization(axis=3)\n",
    "        self.relu2 = tf.keras.layers.Activation('relu')\n",
    "\n",
    "        # Add layer will add together the input and the output\n",
    "        self.add = tf.keras.layers.Add()\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        \"\"\"Performs forward pass on the given input.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor):      The input of dimensions (N, H, W, C)\n",
    "            training (bool): Indicates whether the forward pass happens in the training mode\n",
    "\n",
    "        Returns:\n",
    "            out (Tensor): Output data of dim (N, H/2, W/2, out_channels)\n",
    "        \"\"\"\n",
    "        x_skip = self.skip1(x)                    # prepare to add the input to the output\n",
    "        x = self.relu1(self.norm1(self.conv1(x))) # pass input through the first layer\n",
    "        x = self.norm2(self.conv2(x))             # pass input through the second layer (without ReLU)\n",
    "        out = self.relu2(self.add([x, x_skip]))   # perform ReLU on the processed input added with the raw input\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W4l_Ityxmde6",
    "outputId": "1749d277-95cf-4ad9-b8ca-7a7b95e4a9f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ibarsukov\\anaconda3\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:184: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ibarsukov\\anaconda3\\lib\\site-packages\\keras\\src\\layers\\layer.py:361: UserWarning: `build()` was called on layer 'custom_conv_net_1', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 4.625760078430176, Accuracy: 14.0625, Val Loss: 115.57540130615234, Val Accuracy: 10.699999809265137\n",
      "Iteration 100, Epoch 1, Loss: 0.42999720573425293, Accuracy: 87.99505615234375, Val Loss: 61.415428161621094, Val Accuracy: 22.400001525878906\n",
      "Iteration 200, Epoch 1, Loss: 0.28719526529312134, Accuracy: 91.72108459472656, Val Loss: 41.06241989135742, Val Accuracy: 44.733333587646484\n",
      "Iteration 300, Epoch 1, Loss: 0.22378984093666077, Accuracy: 93.4437255859375, Val Loss: 30.89006233215332, Val Accuracy: 55.900001525878906\n",
      "Iteration 400, Epoch 1, Loss: 0.1853756606578827, Accuracy: 94.54489135742188, Val Loss: 24.7504940032959, Val Accuracy: 63.3800048828125\n",
      "Iteration 500, Epoch 1, Loss: 0.1661100834608078, Accuracy: 95.0692367553711, Val Loss: 20.65190887451172, Val Accuracy: 68.69999694824219\n",
      "Iteration 600, Epoch 1, Loss: 0.15070758759975433, Accuracy: 95.53868865966797, Val Loss: 17.7447566986084, Val Accuracy: 71.8857192993164\n",
      "Iteration 700, Epoch 1, Loss: 0.13983099162578583, Accuracy: 95.86750793457031, Val Loss: 15.543927192687988, Val Accuracy: 74.875\n",
      "Iteration 800, Epoch 2, Loss: 0.13109341263771057, Accuracy: 96.10265350341797, Val Loss: 13.82922077178955, Val Accuracy: 77.20000457763672\n",
      "Iteration 900, Epoch 2, Loss: 0.12187179923057556, Accuracy: 96.36363220214844, Val Loss: 12.453413963317871, Val Accuracy: 79.25999450683594\n",
      "Iteration 1000, Epoch 2, Loss: 0.11469772458076477, Accuracy: 96.55840301513672, Val Loss: 11.327286720275879, Val Accuracy: 80.96363830566406\n",
      "Iteration 1100, Epoch 2, Loss: 0.10777357220649719, Accuracy: 96.75752258300781, Val Loss: 10.387545585632324, Val Accuracy: 82.4000015258789\n",
      "Iteration 1200, Epoch 2, Loss: 0.10139895230531693, Accuracy: 96.95991516113281, Val Loss: 9.600992202758789, Val Accuracy: 83.4000015258789\n",
      "Iteration 1300, Epoch 2, Loss: 0.09675919264554977, Accuracy: 97.09634399414062, Val Loss: 8.926047325134277, Val Accuracy: 84.28571319580078\n",
      "Iteration 1400, Epoch 2, Loss: 0.09341399371623993, Accuracy: 97.2010269165039, Val Loss: 8.345230102539062, Val Accuracy: 84.93999481201172\n",
      "Iteration 1500, Epoch 2, Loss: 0.08959148824214935, Accuracy: 97.3136215209961, Val Loss: 7.8282012939453125, Val Accuracy: 85.71875\n",
      "Iteration 1600, Epoch 3, Loss: 0.08675020933151245, Accuracy: 97.39299011230469, Val Loss: 7.374574184417725, Val Accuracy: 86.35293579101562\n",
      "Iteration 1700, Epoch 3, Loss: 0.08347994089126587, Accuracy: 97.48382568359375, Val Loss: 6.9778337478637695, Val Accuracy: 86.70555114746094\n",
      "Iteration 1800, Epoch 3, Loss: 0.08034250140190125, Accuracy: 97.57672882080078, Val Loss: 6.618066310882568, Val Accuracy: 87.22105407714844\n",
      "Iteration 1900, Epoch 3, Loss: 0.0772993266582489, Accuracy: 97.66560363769531, Val Loss: 6.289548397064209, Val Accuracy: 87.78499603271484\n",
      "Iteration 2000, Epoch 3, Loss: 0.0746534988284111, Accuracy: 97.75340270996094, Val Loss: 5.993778705596924, Val Accuracy: 88.25238037109375\n",
      "Iteration 2100, Epoch 3, Loss: 0.07226752489805222, Accuracy: 97.8209457397461, Val Loss: 5.725268840789795, Val Accuracy: 88.67727661132812\n",
      "Iteration 2200, Epoch 3, Loss: 0.07047013193368912, Accuracy: 97.87666320800781, Val Loss: 5.4817094802856445, Val Accuracy: 89.06521606445312\n",
      "Iteration 2300, Epoch 4, Loss: 0.06847617030143738, Accuracy: 97.93534851074219, Val Loss: 5.256294250488281, Val Accuracy: 89.4375\n",
      "Iteration 2400, Epoch 4, Loss: 0.06702335923910141, Accuracy: 97.97645568847656, Val Loss: 5.052010536193848, Val Accuracy: 89.70000457763672\n",
      "Iteration 2500, Epoch 4, Loss: 0.06551960855722427, Accuracy: 98.01990509033203, Val Loss: 4.862502098083496, Val Accuracy: 89.98461151123047\n",
      "Iteration 2600, Epoch 4, Loss: 0.06399250775575638, Accuracy: 98.064208984375, Val Loss: 4.6852593421936035, Val Accuracy: 90.25926208496094\n",
      "Iteration 2700, Epoch 4, Loss: 0.06218407303094864, Accuracy: 98.11912536621094, Val Loss: 4.520018577575684, Val Accuracy: 90.54643249511719\n",
      "Iteration 2800, Epoch 4, Loss: 0.06073823943734169, Accuracy: 98.15840148925781, Val Loss: 4.366344451904297, Val Accuracy: 90.81378936767578\n",
      "Iteration 2900, Epoch 4, Loss: 0.05952777713537216, Accuracy: 98.19334411621094, Val Loss: 4.227404594421387, Val Accuracy: 90.96666717529297\n",
      "Iteration 3000, Epoch 4, Loss: 0.05859466642141342, Accuracy: 98.22441101074219, Val Loss: 4.094111442565918, Val Accuracy: 91.17096710205078\n",
      "Iteration 3100, Epoch 5, Loss: 0.05725761502981186, Accuracy: 98.26282501220703, Val Loss: 3.9684436321258545, Val Accuracy: 91.39999389648438\n",
      "Iteration 3200, Epoch 5, Loss: 0.05615103244781494, Accuracy: 98.2946548461914, Val Loss: 3.852822780609131, Val Accuracy: 91.53939819335938\n",
      "Iteration 3300, Epoch 5, Loss: 0.05520658195018768, Accuracy: 98.32218933105469, Val Loss: 3.743849992752075, Val Accuracy: 91.69117736816406\n",
      "Iteration 3400, Epoch 5, Loss: 0.05419202148914337, Accuracy: 98.34993743896484, Val Loss: 3.6389410495758057, Val Accuracy: 91.87714385986328\n",
      "Iteration 3500, Epoch 5, Loss: 0.05310037359595299, Accuracy: 98.38280487060547, Val Loss: 3.543459892272949, Val Accuracy: 91.99166870117188\n",
      "Iteration 3600, Epoch 5, Loss: 0.05211741477251053, Accuracy: 98.41253662109375, Val Loss: 3.4509105682373047, Val Accuracy: 92.13243103027344\n",
      "Iteration 3700, Epoch 5, Loss: 0.05135098472237587, Accuracy: 98.43475341796875, Val Loss: 3.3667595386505127, Val Accuracy: 92.17631530761719\n",
      "Iteration 3800, Epoch 5, Loss: 0.050631724298000336, Accuracy: 98.45661926269531, Val Loss: 3.2828822135925293, Val Accuracy: 92.32051086425781\n",
      "Iteration 3900, Epoch 6, Loss: 0.049840521067380905, Accuracy: 98.48123168945312, Val Loss: 3.2031121253967285, Val Accuracy: 92.43999481201172\n",
      "Iteration 4000, Epoch 6, Loss: 0.04898794740438461, Accuracy: 98.50514221191406, Val Loss: 3.128786087036133, Val Accuracy: 92.53658294677734\n",
      "Iteration 4100, Epoch 6, Loss: 0.048169393092393875, Accuracy: 98.52979278564453, Val Loss: 3.0565788745880127, Val Accuracy: 92.6547622680664\n",
      "Iteration 4200, Epoch 6, Loss: 0.04739474505186081, Accuracy: 98.55140686035156, Val Loss: 2.9874892234802246, Val Accuracy: 92.77442169189453\n",
      "Iteration 4300, Epoch 6, Loss: 0.046570222824811935, Accuracy: 98.57456970214844, Val Loss: 2.9215264320373535, Val Accuracy: 92.88409423828125\n",
      "Iteration 4400, Epoch 6, Loss: 0.045856740325689316, Accuracy: 98.59666442871094, Val Loss: 2.8587005138397217, Val Accuracy: 93.0\n",
      "Iteration 4500, Epoch 6, Loss: 0.04527277499437332, Accuracy: 98.6104965209961, Val Loss: 2.7984747886657715, Val Accuracy: 93.11521911621094\n",
      "Iteration 4600, Epoch 7, Loss: 0.04459155723452568, Accuracy: 98.62938690185547, Val Loss: 2.7401535511016846, Val Accuracy: 93.2276611328125\n",
      "Iteration 4700, Epoch 7, Loss: 0.04397236555814743, Accuracy: 98.64691162109375, Val Loss: 2.6844749450683594, Val Accuracy: 93.32917022705078\n",
      "Iteration 4800, Epoch 7, Loss: 0.0433928519487381, Accuracy: 98.66436767578125, Val Loss: 2.631382465362549, Val Accuracy: 93.43061828613281\n",
      "Iteration 4900, Epoch 7, Loss: 0.04274098202586174, Accuracy: 98.68301391601562, Val Loss: 2.579892873764038, Val Accuracy: 93.53600311279297\n",
      "Iteration 5000, Epoch 7, Loss: 0.042173076421022415, Accuracy: 98.70061492919922, Val Loss: 2.531090497970581, Val Accuracy: 93.6294174194336\n",
      "Iteration 5100, Epoch 7, Loss: 0.04169483110308647, Accuracy: 98.71200561523438, Val Loss: 2.484933614730835, Val Accuracy: 93.69999694824219\n",
      "Iteration 5200, Epoch 7, Loss: 0.0412769578397274, Accuracy: 98.72565460205078, Val Loss: 2.439582586288452, Val Accuracy: 93.7811279296875\n",
      "Iteration 5300, Epoch 7, Loss: 0.04082680493593216, Accuracy: 98.7387924194336, Val Loss: 2.396138906478882, Val Accuracy: 93.86481475830078\n",
      "Iteration 5400, Epoch 8, Loss: 0.04029487445950508, Accuracy: 98.7545394897461, Val Loss: 2.3542985916137695, Val Accuracy: 93.92363739013672\n",
      "Iteration 5500, Epoch 8, Loss: 0.03983065113425255, Accuracy: 98.76952362060547, Val Loss: 2.313585042953491, Val Accuracy: 94.0\n",
      "Iteration 5600, Epoch 8, Loss: 0.039377909153699875, Accuracy: 98.7825698852539, Val Loss: 2.2746455669403076, Val Accuracy: 94.0649185180664\n",
      "Iteration 5700, Epoch 8, Loss: 0.0388408862054348, Accuracy: 98.7976303100586, Val Loss: 2.23712158203125, Val Accuracy: 94.12931060791016\n",
      "Iteration 5800, Epoch 8, Loss: 0.03826894983649254, Accuracy: 98.8145980834961, Val Loss: 2.200413703918457, Val Accuracy: 94.19660949707031\n",
      "Iteration 5900, Epoch 8, Loss: 0.03772563487291336, Accuracy: 98.83151245117188, Val Loss: 2.164921760559082, Val Accuracy: 94.25833129882812\n",
      "Iteration 6000, Epoch 8, Loss: 0.03728112205862999, Accuracy: 98.84369659423828, Val Loss: 2.1313440799713135, Val Accuracy: 94.30655670166016\n",
      "Iteration 6100, Epoch 8, Loss: 0.03689498081803322, Accuracy: 98.85446166992188, Val Loss: 2.098280429840088, Val Accuracy: 94.3725814819336\n",
      "Iteration 6200, Epoch 9, Loss: 0.036524511873722076, Accuracy: 98.86581420898438, Val Loss: 2.066796064376831, Val Accuracy: 94.41905212402344\n",
      "Iteration 6300, Epoch 9, Loss: 0.03632358834147453, Accuracy: 98.87216186523438, Val Loss: 2.036756753921509, Val Accuracy: 94.4593734741211\n",
      "Iteration 6400, Epoch 9, Loss: 0.03586805611848831, Accuracy: 98.88685607910156, Val Loss: 2.0069985389709473, Val Accuracy: 94.50615692138672\n",
      "Iteration 6500, Epoch 9, Loss: 0.03548336401581764, Accuracy: 98.8984603881836, Val Loss: 1.9776158332824707, Val Accuracy: 94.56515502929688\n",
      "Iteration 6600, Epoch 9, Loss: 0.03508276119828224, Accuracy: 98.90876007080078, Val Loss: 1.9499274492263794, Val Accuracy: 94.5985107421875\n",
      "Iteration 6700, Epoch 9, Loss: 0.03477821126580238, Accuracy: 98.91851806640625, Val Loss: 1.9227228164672852, Val Accuracy: 94.63382720947266\n",
      "Iteration 6800, Epoch 9, Loss: 0.03446072340011597, Accuracy: 98.92753601074219, Val Loss: 1.8966621160507202, Val Accuracy: 94.68260955810547\n",
      "Iteration 6900, Epoch 10, Loss: 0.03416003659367561, Accuracy: 98.9369125366211, Val Loss: 1.8707815408706665, Val Accuracy: 94.72856903076172\n",
      "Iteration 7000, Epoch 10, Loss: 0.03380892425775528, Accuracy: 98.94786071777344, Val Loss: 1.845577597618103, Val Accuracy: 94.77183532714844\n",
      "Iteration 7100, Epoch 10, Loss: 0.03344779089093208, Accuracy: 98.95915985107422, Val Loss: 1.8208658695220947, Val Accuracy: 94.82638549804688\n",
      "Iteration 7200, Epoch 10, Loss: 0.03318467363715172, Accuracy: 98.96688842773438, Val Loss: 1.7974387407302856, Val Accuracy: 94.86438751220703\n",
      "Iteration 7300, Epoch 10, Loss: 0.03285733982920647, Accuracy: 98.97655487060547, Val Loss: 1.773811936378479, Val Accuracy: 94.91486358642578\n",
      "Iteration 7400, Epoch 10, Loss: 0.03259655833244324, Accuracy: 98.98383331298828, Val Loss: 1.7511639595031738, Val Accuracy: 94.95333099365234\n",
      "Iteration 7500, Epoch 10, Loss: 0.03227813541889191, Accuracy: 98.99322509765625, Val Loss: 1.7294951677322388, Val Accuracy: 94.97894287109375\n",
      "Iteration 7600, Epoch 10, Loss: 0.03197029232978821, Accuracy: 99.00276947021484, Val Loss: 1.7081637382507324, Val Accuracy: 95.0064926147461\n"
     ]
    }
   ],
   "source": [
    "class CustomConvNet(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(CustomConvNet, self).__init__()\n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        self.model = ResNet()\n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################\n",
    "\n",
    "    def call(self, input_tensor, training=False):\n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        x = self.model.call(input_tensor, training)\n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################\n",
    "        return x\n",
    "\n",
    "\n",
    "print_every = 100\n",
    "num_epochs = 10\n",
    "\n",
    "model = CustomConvNet()\n",
    "\n",
    "def model_init_fn():\n",
    "    return CustomConvNet()\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    learning_rate = 1e-3\n",
    "    return tf.keras.optimizers.Adam(learning_rate)\n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn, num_epochs=num_epochs, is_training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nREnkxK3mde7"
   },
   "source": [
    "Опишите все эксперименты, результаты. Сделайте выводы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kWa5BMuIroCi"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Y5OZJJumde7"
   },
   "source": [
    "Была разработана архитектура нейронной сети, которая включает в себя предварительную обработку входных данных через сверточные слои с последующей нормализацией и применением функции активации ReLU. Затем следует блок, состоящий из нескольких слоев, включающих \"skip\" соединения, повторяющийся несколько раз. Наконец, выполнено глобальное усреднение пула перед применением полносвязного слоя и функции активации softmax. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
